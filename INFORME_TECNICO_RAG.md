# Informe T√©cnico: Arquitectura RAG del Chatbot Conversacional con IA Local

**Proyecto:** Gemma3-3n Whisperer ‚Äî Audio Chatbot with Model Selector  
**Versi√≥n:** MVP 1.0  
**Fecha:** Febrero 2026  
**Alcance:** Documentaci√≥n t√©cnica completa del marco RAG (Retrieval-Augmented Generation) integrado al chatbot conversacional multimodal

---

## 1. Resumen Ejecutivo

Este documento describe la arquitectura, el dise√±o, la implementaci√≥n y el funcionamiento del sistema de Generaci√≥n Aumentada por Recuperaci√≥n (RAG) integrado en un chatbot conversacional que opera completamente en infraestructura local. El sistema permite que los modelos de lenguaje (LLM) respondan preguntas fundamentadas en documentos corporativos reales, eliminando alucinaciones sobre temas espec√≠ficos del dominio y priorizando el conocimiento verificado sobre la generaci√≥n especulativa.

El MVP implementa un pipeline completo que abarca desde la ingesta de documentos en m√∫ltiples formatos (.docx, .xlsx, .pdf, .txt, .md) hasta la presentaci√≥n de resultados enriquecidos con fuentes citadas en la interfaz de usuario. Todo el procesamiento ocurre localmente sin dependencias de servicios cloud, lo cual garantiza la privacidad de los datos corporativos y elimina costos recurrentes de APIs externas.

La arquitectura resultante combina cinco subsistemas interconectados: procesamiento de documentos con chunking inteligente, indexaci√≥n vectorial persistente con ChromaDB, generaci√≥n de embeddings sem√°nticos con Ollama, enrutamiento inteligente de consultas entre modelos especializados, y una interfaz React que expone las fuentes de conocimiento utilizadas para cada respuesta.

---

## 2. Fundamentos y Justificaci√≥n del Enfoque RAG

### 2.1 Problema que Resuelve

Los modelos de lenguaje grandes, aun cuando son capaces de generar texto coherente y contextualmente plausible, presentan una limitaci√≥n fundamental: su conocimiento est√° congelado en el momento del entrenamiento. Cuando un usuario pregunta por procedimientos internos, recetas espec√≠ficas o protocolos operativos que nunca formaron parte de los datos de entrenamiento, el modelo tiene dos opciones: admitir desconocimiento o fabricar una respuesta veros√≠mil pero incorrecta. En entornos productivos donde la precisi√≥n es cr√≠tica ‚Äî como la estandarizaci√≥n de recetas en un restaurante ‚Äî esta segunda opci√≥n resulta inaceptable.

RAG resuelve este problema inyectando conocimiento externo verificado directamente en el contexto del modelo antes de que genere su respuesta. En lugar de depender exclusivamente de los par√°metros aprendidos durante el entrenamiento, el modelo recibe fragmentos relevantes de documentos corporativos como parte de su prompt, lo que le permite fundamentar sus respuestas en informaci√≥n real y citar las fuentes utilizadas.

### 2.2 Por Qu√© RAG y No Fine-Tuning

La alternativa cl√°sica a RAG es el fine-tuning (ajuste fino) del modelo con datos propios. Se descart√≥ esta opci√≥n por tres razones concretas. Primero, el fine-tuning requiere conjuntos de datos de entrenamiento curados en formato pregunta-respuesta, lo cual demanda un esfuerzo significativo de preparaci√≥n que no escala cuando los documentos cambian frecuentemente. Segundo, cada actualizaci√≥n de contenido requerir√≠a reentrenar el modelo, un proceso que consume horas de c√≥mputo GPU y genera una nueva versi√≥n del modelo que debe desplegarse. Tercero, RAG preserva la trazabilidad: cada respuesta puede acompa√±arse de la fuente documental exacta que la sustenta, algo que el fine-tuning no ofrece porque el conocimiento se diluye en los pesos del modelo.

### 2.3 Decisi√≥n de Infraestructura Local

Toda la infraestructura opera localmente mediante Ollama para inferencia LLM y embeddings, ChromaDB como base de datos vectorial embebida, y Whisper para transcripci√≥n de audio. Esta decisi√≥n responde a tres factores: la naturaleza confidencial de los documentos corporativos que no deben transitar por APIs externas, la eliminaci√≥n de costos recurrentes por token que generan servicios como OpenAI o Anthropic, y la capacidad de operar sin conexi√≥n a internet una vez que los modelos est√°n descargados.

---

## 3. Arquitectura del Sistema

### 3.1 Vista General

El sistema sigue una arquitectura de tres capas con un flujo de datos unidireccional desde la interfaz de usuario hasta los modelos de IA, enriquecido en cada paso con contexto recuperado de la base de conocimiento.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CAPA DE PRESENTACI√ìN                         ‚îÇ
‚îÇ  React 19 + Vite 7 + TailwindCSS 4                             ‚îÇ
‚îÇ  Audio Recording ‚îÄ Text Input ‚îÄ Image Upload ‚îÄ TTS Playback    ‚îÇ
‚îÇ  Renderizado Markdown ‚îÄ Panel de Fuentes RAG colapsable         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ HTTP/JSON (localhost:5173 ‚Üí :8000)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CAPA DE APLICACI√ìN                           ‚îÇ
‚îÇ  FastAPI (gemma_server.py)                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ  ‚îÇ Whisper  ‚îÇ  ‚îÇ Smart Router ‚îÇ  ‚îÇ RAG Search  ‚îÇ               ‚îÇ
‚îÇ  ‚îÇ STT      ‚îÇ  ‚îÇ qwen2.5:0.5b ‚îÇ  ‚îÇ ChromaDB   ‚îÇ               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ       ‚îÇ               ‚îÇ                 ‚îÇ                       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ
‚îÇ           ‚îÇ   Ollama LLM Backend    ‚îÇ                           ‚îÇ
‚îÇ           ‚îÇ gemma3:4b ‚îÄ‚îÄ phi4:latest‚îÇ                           ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CAPA DE DATOS                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ ChromaDB    ‚îÇ  ‚îÇ .index.json   ‚îÇ  ‚îÇ documents/     ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ (rag_data/) ‚îÇ  ‚îÇ File Tracking ‚îÇ  ‚îÇ Source Files   ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ SQLite+HNSW ‚îÇ  ‚îÇ Hash Registry ‚îÇ  ‚îÇ .docx .xlsx .md‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Componentes y Responsabilidades

El sistema se compone de seis m√≥dulos principales, cada uno con un √°mbito de responsabilidad claramente delimitado.

**`document_processors.py`** se encarga exclusivamente de la extracci√≥n y fragmentaci√≥n de texto desde archivos fuente. Implementa procesadores especializados por formato (Word, Excel, PDF, texto plano y Markdown) y aplica estrategias de chunking adaptadas a la estructura de cada tipo de documento. Su salida son objetos `DocumentChunk` que encapsulan el contenido textual junto con metadatos de origen.

**`rag_module.py`** constituye el n√∫cleo del sistema RAG. Define la clase `OllamaEmbeddingFunction` que adapta la API de embeddings de Ollama al protocolo que ChromaDB requiere, la clase `RAGSystem` que abstrae todas las operaciones de la base de datos vectorial, y funciones auxiliares para gesti√≥n de √≠ndices y construcci√≥n de prompts enriquecidos con contexto.

**`rag_admin.py`** proporciona una interfaz de l√≠nea de comandos (CLI) para administrar la base de conocimiento. Permite sincronizar documentos, agregar o eliminar fuentes, buscar contenido, visualizar estad√≠sticas y reconstruir el √≠ndice completo. Opera como herramienta de mantenimiento independiente del servidor web.

**`gemma_server.py`** es el servidor FastAPI que orquesta todo el flujo: recibe peticiones HTTP del frontend, transcribe audio con Whisper cuando corresponde, consulta la base de conocimiento RAG, construye prompts enriquecidos con contexto documental, enruta consultas al modelo apropiado mediante clasificaci√≥n autom√°tica, y retorna respuestas junto con los fragmentos de fuentes utilizados.

**`gemma-chatbot-ui/src/App.jsx`** implementa la interfaz de usuario completa en React. Gestiona grabaci√≥n de audio, entrada de texto, carga de im√°genes, selecci√≥n de modelos, visualizaci√≥n de conversaciones con renderizado Markdown, reproducci√≥n TTS, y un panel colapsable que muestra las fuentes RAG asociadas a cada respuesta del modelo.

**`rag_data/`** almacena de forma persistente la base de datos vectorial ChromaDB (SQLite + √≠ndices HNSW) y el archivo de tracking `.index.json` que registra los hashes MD5 de cada archivo indexado para detecci√≥n incremental de cambios.

### 3.3 Flujo de Datos Completo

Cuando un usuario env√≠a una consulta ‚Äî ya sea por voz o texto ‚Äî el sistema ejecuta la siguiente secuencia:

1. Si la entrada es audio, Whisper la transcribe a texto en espa√±ol con par√°metros optimizados (temperatura 0, sin condicionamiento sobre texto previo, deshabilitando FP16 para mayor precisi√≥n).

2. El texto de la consulta se env√≠a al m√≥dulo RAG, que genera un embedding vectorial de 768 dimensiones utilizando el modelo `nomic-embed-text` a trav√©s de la API `/api/embed` de Ollama.

3. ChromaDB ejecuta una b√∫squeda por similitud coseno (HNSW) contra los embeddings almacenados de los chunks documentales, retornando los `RAG_TOP_K` resultados m√°s cercanos (por defecto 3).

4. Los resultados se filtran por relevancia m√≠nima (`RAG_MIN_RELEVANCE = 0.3`). Solo los fragmentos que superan el umbral se incorporan al contexto.

5. Si el modo de enrutamiento es "auto", el modelo ligero `qwen2.5:0.5b` clasifica la consulta como "math" o "chat" para dirigirla al modelo especializado correspondiente (`phi4:latest` para matem√°ticas, `gemma3:4b` para conversaci√≥n general).

6. Se construye el system prompt combinando el contexto interno del asistente, los fragmentos RAG relevantes (marcados como "PRIORIDAD ALTA"), el resumen de conversaci√≥n anterior si existe, y el contexto personalizado del usuario desde el frontend.

7. El modelo LLM genera la respuesta fundamentada en el contexto documental inyectado.

8. La respuesta se retorna al frontend junto con los `rag_chunks` (fuente, secci√≥n, relevancia y contenido truncado a 500 caracteres), que se renderizan en un panel colapsable bajo cada mensaje del asistente.

---

## 4. Pipeline de Procesamiento de Documentos

### 4.1 Formatos Soportados y Estrategias de Extracci√≥n

El m√≥dulo `document_processors.py` implementa procesadores especializados para cinco familias de formatos de archivo, registrados en el diccionario `PROCESSORS` que mapea extensiones a funciones de procesamiento.

**Archivos de texto plano y Markdown (.txt, .md):** La funci√≥n `process_text_file` detecta la estructura del documento analizando encabezados Markdown con la expresi√≥n regular `^(#{1,6})\s+(.+)$`. Cada secci√≥n definida por un encabezado se convierte en un chunk independiente si su longitud no excede `DEFAULT_CHUNK_SIZE` (1000 caracteres). Las secciones m√°s extensas se subdividen mediante `chunk_text`, que prioriza la divisi√≥n por p√°rrafos dobles (`\n\n`) para preservar la coherencia sem√°ntica de cada fragmento. Los chunks resultantes se prefijan con el encabezado de la secci√≥n original para mantener el contexto estructural.

**Documentos Word (.docx):** La funci√≥n `process_word_file` utiliza la biblioteca `python-docx` para iterar sobre los elementos XML del cuerpo del documento. Detecta encabezados mediante el estilo del p√°rrafo (`para.style.name.startswith('Heading')`) y agrupa el contenido subsecuente bajo cada secci√≥n. Las tablas encontradas en el documento se convierten a formato Markdown mediante `table_to_markdown`, que genera una representaci√≥n textual con separadores de columnas (`|`) y l√≠nea de encabezado (`|---|`), facilitando la comprensi√≥n por parte del modelo de lenguaje.

**Hojas de c√°lculo Excel (.xlsx, .xls):** La funci√≥n `process_excel_file` emplea `pandas` con el motor `openpyxl` para leer cada hoja del documento. Soporta dos estrategias de chunking configurables: el modo "markdown" convierte la tabla completa (o fragmentos de `rows_per_chunk` filas) a formato Markdown tabulado mediante `DataFrame.to_markdown()`, mientras que el modo "rows" genera un chunk narrativo por cada fila con el patr√≥n "Columna: Valor. Columna: Valor.", lo cual resulta m√°s adecuado para tablas de registros independientes como inventarios o listas de ingredientes.

**Documentos PDF (.pdf):** La funci√≥n `process_pdf_file` utiliza `pypdf` (con fallback a `PyPDF2` para compatibilidad) para extraer texto p√°gina por p√°gina. El texto extra√≠do se limpia mediante `clean_pdf_text`, que normaliza espacios m√∫ltiples, consolida saltos de l√≠nea consecutivos y reconecta palabras divididas por guiones al final de l√≠nea (`-\n` ‚Üí concatenaci√≥n directa). Los metadatos incluyen el n√∫mero total de p√°ginas y la posici√≥n del chunk dentro del documento.

### 4.2 Estrategia de Chunking

El chunking es una de las decisiones de dise√±o m√°s cr√≠ticas en un sistema RAG porque determina la granularidad de la informaci√≥n recuperable. Chunks demasiado grandes diluyen la relevancia sem√°ntica y consumen tokens de contexto innecesariamente; chunks demasiado peque√±os pierden coherencia y dificultan que el modelo comprenda el contexto completo de una instrucci√≥n o procedimiento.

La implementaci√≥n actual utiliza un enfoque h√≠brido con dos niveles. En el primer nivel, el documento se divide por su estructura natural: encabezados en archivos de texto y Word, hojas en archivos Excel, y p√°ginas en archivos PDF. En el segundo nivel, cada secci√≥n resultante se eval√∫a contra el tama√±o m√°ximo configurado (`DEFAULT_CHUNK_SIZE = 1000` caracteres, aproximadamente 250-300 tokens). Las secciones que exceden este l√≠mite se subdividen por p√°rrafos dobles, con un solapamiento configurable de `DEFAULT_OVERLAP = 100` caracteres entre chunks consecutivos para evitar la p√©rdida de contexto en las fronteras.

Cada chunk generado se encapsula en un objeto `DocumentChunk` con tres atributos: `content` (el texto fragmentado, prefijado con el encabezado de secci√≥n), `metadata` (un diccionario con el archivo fuente, la secci√≥n, el tipo de procesador y atributos espec√≠ficos del formato), y `chunk_id` (un identificador √∫nico construido como `{nombre_archivo}_{n√∫mero_secuencial}`).

### 4.3 Modelo de Datos del Chunk

La estructura de metadatos var√≠a seg√∫n el procesador pero mantiene un esquema base com√∫n:

| Campo        | Tipo   | Descripci√≥n                                  | Presente en     |
|-------------|--------|----------------------------------------------|-----------------|
| `source`     | str    | Ruta relativa del archivo fuente             | Todos           |
| `section`    | str    | Encabezado o nombre de secci√≥n               | Todos           |
| `type`       | str    | Tipo de procesador (text, word, excel, pdf)  | Todos           |
| `indexed_at` | str    | Timestamp ISO 8601 de indexaci√≥n             | Post-ingesta    |
| `sheet`      | str    | Nombre de la hoja Excel                      | Excel           |
| `rows`       | int    | Cantidad de filas en el chunk                | Excel           |
| `columns`    | list   | Nombres de columnas                          | Excel           |
| `total_pages`| int    | Total de p√°ginas del PDF                     | PDF             |
| `chunk`      | int    | N√∫mero de chunk dentro del PDF               | PDF             |
| `part`       | int    | N√∫mero de parte cuando se subdivide secci√≥n  | Texto/Word      |

---

## 5. Motor de Embeddings y Base Vectorial

### 5.1 Modelo de Embeddings: nomic-embed-text

El sistema utiliza `nomic-embed-text` como modelo de embeddings, ejecutado localmente a trav√©s de Ollama. Este modelo genera vectores de 768 dimensiones que capturan la sem√°ntica del texto en un espacio donde la proximidad entre vectores refleja la similitud de significado entre los textos que representan.

La elecci√≥n de `nomic-embed-text` se fundamenta en su equilibrio entre calidad y eficiencia: ocupa aproximadamente 275 MB de almacenamiento, genera embeddings en milisegundos para textos cortos, y produce representaciones sem√°nticas competitivas con modelos significativamente m√°s grandes. Al ejecutarse localmente mediante Ollama, elimina la latencia de red y los costos por token que implicar√≠a utilizar servicios como la API de embeddings de OpenAI.

### 5.2 Adaptador OllamaEmbeddingFunction

La clase `OllamaEmbeddingFunction` act√∫a como adaptador entre la API REST de Ollama y el protocolo de funciones de embedding que ChromaDB requiere. Este adaptador fue uno de los componentes que requiri√≥ mayor iteraci√≥n durante el desarrollo debido a incompatibilidades entre las interfaces.

ChromaDB 1.5 define el protocolo `EmbeddingFunction[D]` que requiere que el m√©todo `__call__` acepte un par√°metro `input` de tipo gen√©rico `D` y retorne `List[np.ndarray]` (alias `Embeddings`). Adicionalmente, expone los m√©todos `embed_query` y `embed_documents` para diferenciar el procesamiento de consultas y documentos.

El desaf√≠o t√©cnico principal surgi√≥ de que ChromaDB 1.5 pasa el input a `embed_query` como una **lista** (e.g., `input=['texto de b√∫squeda']`) en lugar de como una cadena simple, y que `__call__` recibe los textos como listas anidadas (e.g., `[['texto1'], ['texto2']]`). Simult√°neamente, la API de Ollama cambi√≥ su endpoint de `/api/embeddings` (con payload `{"prompt": text}`) a `/api/embed` (con payload `{"input": text_or_list}`), y el formato de respuesta de `data["embedding"]` a `data["embeddings"]`. El resultado retornado debe ser `List[np.ndarray]`, no `List[List[float]]`, ya que ChromaDB 1.5 valida los tipos estrictamente.

La implementaci√≥n final resuelve estas incompatibilidades de la siguiente manera:

```python
def __call__(self, input) -> List[np.ndarray]:
    # Flatten listas anidadas de ChromaDB 1.5: [['text1']] ‚Üí ['text1']
    flat_texts = []
    if input and isinstance(input[0], list):
        for item in input:
            flat_texts.append(item[0] if item else "")
    else:
        flat_texts = list(input)
    raw = self._get_embeddings(flat_texts)
    return [np.array(e, dtype=np.float32) for e in raw]

def embed_query(self, input="", query="", **kwargs) -> List[np.ndarray]:
    # ChromaDB 1.5 pasa input como lista: embed_query(input=['text'])
    if isinstance(input, list):
        texts = [t if isinstance(t, str) else str(t) for t in input]
    else:
        texts = [input or query]
    raw = self._get_embeddings(texts)
    return [np.array(e, dtype=np.float32) for e in raw]
```

El m√©todo interno `_get_embeddings` centraliza la comunicaci√≥n HTTP con Ollama, enviando las listas de texto ya normalizadas al endpoint `/api/embed` y retornando los vectores crudos. Implementa un mecanismo de fallback que produce vectores nulos de 768 dimensiones (`[0.0] * 768`) cuando el servicio no est√° disponible, evitando que errores de conectividad colapsen el pipeline completo.

### 5.3 ChromaDB: Base de Datos Vectorial

ChromaDB opera como la base de datos vectorial del sistema, configurada en modo persistente con almacenamiento en el directorio `rag_data/`. La persistencia se logra mediante un backend SQLite que almacena los documentos, metadatos e identificadores, complementado con √≠ndices HNSW (Hierarchical Navigable Small World) que permiten b√∫squedas aproximadas de vecinos m√°s cercanos en tiempo sublineal.

La colecci√≥n se configura con espacio m√©trico de similitud coseno (`"hnsw:space": "cosine"`), que mide la similitud entre vectores por el √°ngulo que forman en lugar de la distancia euclidiana. Esta m√©trica es est√°ndar para embeddings de texto porque normaliza la magnitud de los vectores, concentr√°ndose exclusivamente en la direcci√≥n ‚Äî es decir, en la sem√°ntica ‚Äî de las representaciones.

La clase `RAGSystem` encapsula todas las operaciones sobre ChromaDB:

La **inicializaci√≥n** crea el directorio de datos si no existe, instancia un `PersistentClient` con telemetr√≠a deshabilitada, configura la funci√≥n de embeddings, y obtiene o crea la colecci√≥n con nombre `knowledge_base`.

La **adici√≥n de documentos** acepta listas paralelas de textos, identificadores √∫nicos y metadatos opcionales. Si no se proporcionan metadatos, genera autom√°ticamente un diccionario con la marca temporal de indexaci√≥n.

La **b√∫squeda** invoca `collection.query` con `query_texts` (delegando la generaci√≥n de embeddings a ChromaDB internamente), retorna los `n_results` documentos m√°s cercanos, y formatea los resultados a√±adiendo un campo `relevance` calculado como `1 - distance` (dado que la distancia coseno oscila entre 0 ‚Äîid√©nticos‚Äî y 2 ‚Äîopuestos‚Äî, la relevancia resultante var√≠a entre -1 y 1, aunque en la pr√°ctica los valores para documentos relacionados caen entre 0.3 y 0.8).

La **eliminaci√≥n** soporta borrado por fuente (`delete_by_source`, que busca por metadata y elimina todos los chunks asociados a un archivo) y por identificadores espec√≠ficos (`delete_by_ids`).

El **vaciado completo** (`clear`) elimina la colecci√≥n y la recrea, lo cual es necesario durante reconstrucciones del √≠ndice.

### 5.4 Gesti√≥n de √çndice Incremental

El archivo `.index.json` mantiene un registro de todos los archivos procesados con sus hashes MD5 y la cantidad de chunks generados. Este mecanismo permite que la sincronizaci√≥n sea incremental: al ejecutar `rag_admin.py sync`, el sistema compara el hash actual de cada archivo contra el hash registrado. Solo los archivos nuevos (no presentes en el √≠ndice) o modificados (hash diferente) se reprocesan. Los archivos que ya no existen en el directorio `documents/` se eliminan autom√°ticamente de la base de conocimiento. Esta estrategia evita reprocesar colecciones documentales completas cuando solo cambia un archivo, reduciendo significativamente el tiempo de actualizaci√≥n.

---

## 6. Integraci√≥n RAG en el Servidor Backend

### 6.1 Inicializaci√≥n Lazy del Sistema RAG

El servidor inicializa el sistema RAG de forma diferida (lazy loading) mediante la funci√≥n `get_rag_system()`. La primera invocaci√≥n instancia `RAGSystem` y almacena la referencia en una variable global `_rag_system`. Invocaciones subsecuentes retornan la instancia existente sin incurrir en el costo de reconexi√≥n a ChromaDB. Si la inicializaci√≥n falla (por ejemplo, si ChromaDB no est√° instalado), el sistema contin√∫a operando sin RAG, degradando la funcionalidad de forma elegante en lugar de fallar catastr√≥ficamente.

La importaci√≥n del m√≥dulo RAG tambi√©n est√° protegida por un bloque try/except que establece el flag `RAG_AVAILABLE` a `False` si las dependencias no est√°n instaladas, permitiendo que el servidor funcione como chatbot puro sin base de conocimiento.

### 6.2 B√∫squeda y Filtrado de Contexto

La funci√≥n as√≠ncrona `search_rag_context` ejecuta la b√∫squeda RAG y filtra los resultados por relevancia m√≠nima:

```python
async def search_rag_context(query: str) -> tuple[str, List[Dict]]:
    results = rag.search(query, n_results=RAG_TOP_K)  # Top 3
    relevant = [r for r in results if r.get("relevance", 0) >= RAG_MIN_RELEVANCE]  # >= 0.3
```

Los documentos que superan el umbral se formatean como texto estructurado con encabezados que incluyen la fuente, secci√≥n y porcentaje de relevancia. Este texto formateado se inyecta en el system prompt, mientras que los resultados crudos se retornan al frontend como `rag_chunks` para visualizaci√≥n.

### 6.3 Construcci√≥n del System Prompt Enriquecido

La funci√≥n `build_system_prompt` compone el prompt del sistema mediante capas jer√°rquicas de contexto. La capa base es `INTERNAL_CONTEXT`, un prompt est√°tico que define la personalidad y las directrices generales del asistente. Sobre esta base se apilan, en orden de prioridad: el contexto RAG (documentos relevantes), el resumen de conversaci√≥n anterior (si la historia excede 20 mensajes), y el contexto personalizado desde el frontend.

El contexto RAG se marca expl√≠citamente con la instrucci√≥n "PRIORIDAD ALTA" y directivas claras para que el modelo base sus respuestas en los documentos cuando sean relevantes, complemente con conocimiento general solo cuando los documentos no cubran la pregunta, y cite la fuente cuando utilice informaci√≥n de la base de conocimiento. Esta jerarquizaci√≥n expl√≠cita ha demostrado ser necesaria porque los modelos tienden a favorecer su conocimiento param√©trico sobre el contexto inyectado cuando las instrucciones no son suficientemente directivas.

### 6.4 Enrutamiento Inteligente de Consultas

El sistema implementa un mecanismo de enrutamiento que clasifica autom√°ticamente cada consulta para dirigirla al modelo m√°s adecuado. El modelo ultraligero `qwen2.5:0.5b` (~270 MB) opera como clasificador con temperatura 0 (determin√≠stico) y predicci√≥n limitada a 10 tokens. El prompt de clasificaci√≥n le pide que responda con una sola palabra: "math" para consultas que requieren c√°lculos, conversiones o proporciones, y "chat" para consultas conversacionales o informativas.

Las consultas matem√°ticas se dirigen a `phi4:latest` (14B par√°metros, razonamiento avanzado), mientras que las conversacionales van a `gemma3:4b` (balance calidad/tama√±o). El usuario tambi√©n puede seleccionar manualmente un modelo espec√≠fico o deshabilitar el enrutamiento autom√°tico.

Este mecanismo es particularmente relevante para el caso de uso RAG en gastronom√≠a: una pregunta como "¬øcu√°ntos gramos de sal necesito para 5 porciones de pollo juanillo?" involucra tanto la recuperaci√≥n de la receta (RAG + chat) como un c√°lculo proporcional (math), y el sistema puede enrutar la respuesta al modelo que mejor maneje la aritm√©tica implicada.

### 6.5 Endpoints RAG Expuestos

El servidor expone cinco endpoints dedicados a la administraci√≥n RAG:

`GET /rag/status` retorna el estado del sistema incluyendo si est√° habilitado, inicializado, el conteo de documentos y chunks, el modelo de embeddings, la fecha de √∫ltima sincronizaci√≥n y los par√°metros de configuraci√≥n actuales (top_k, min_relevance).

`GET /rag/documents` lista los documentos indexados con sus rutas relativas, cantidad de chunks y fecha de indexaci√≥n.

`POST /rag/search` permite realizar b√∫squedas de prueba contra la base de conocimiento con un query y n√∫mero de resultados configurables, retornando contenido, fuente y relevancia porcentual.

`POST /rag/sync` dispara la sincronizaci√≥n de documentos ejecutando `rag_admin.py sync` como subproceso, y reinicializa la instancia del sistema RAG para cargar los cambios.

Los endpoints principales `/ask` (audio) y `/ask_text` (texto) integran RAG de forma transparente, incluyendo en su respuesta JSON los campos `rag_sources` (lista de fuentes √∫nicas) y `rag_chunks` (fragmentos con contenido, fuente, secci√≥n y relevancia).

---

## 7. Integraci√≥n en el Frontend

### 7.1 Propagaci√≥n de Fuentes RAG

La interfaz React recibe los `rag_chunks` del backend y los propaga a trav√©s de todo el flujo de renderizado de mensajes. La funci√≥n `addReply(text, modelInfo, ragChunks)` acepta tres par√°metros, y los tres call sites del c√≥digo ‚Äî `processAudio` (procesamiento de audio), `submitTextPrompt` (env√≠o de texto), y `resendMessage` (reenv√≠o de mensajes) ‚Äî extraen y propagan `data.rag_chunks` desde la respuesta del servidor.

Cada mensaje del modelo en el estado `conversation` almacena los chunks como propiedad `ragChunks`, lo que permite que las fuentes persistan mientras dure la sesi√≥n de chat y se rendericen junto a la respuesta correspondiente.

### 7.2 Panel de Fuentes Colapsable

Cuando una respuesta del modelo incluye fuentes RAG, se renderiza un componente `<details>` colapsable inmediatamente debajo del texto de respuesta. El trigger muestra "üìö Fuentes (N)" indicando la cantidad de fragmentos utilizados. Al expandir, se presenta cada chunk en una tarjeta con fondo oscuro semitransparente (`bg-gray-800/80`) que incluye:

El nombre del archivo fuente con estilo destacado en gris claro, posicionado a la izquierda de la tarjeta. El porcentaje de relevancia en color √≠ndigo a la derecha, calculado como `(chunk.relevance * 100).toFixed(0)`. El nombre de la secci√≥n en cursiva cuando est√° disponible. El contenido del fragmento, truncado a 300 caracteres en la interfaz (el servidor ya lo trunca a 500) con indicador de elipsis.

Esta implementaci√≥n permite que el usuario verifique las fuentes de cada respuesta sin saturar la interfaz: las fuentes permanecen ocultas por defecto y solo se revelan cuando el usuario las solicita expl√≠citamente.

---

## 8. Herramienta de Administraci√≥n CLI

### 8.1 Comandos Disponibles

El script `rag_admin.py` expone ocho comandos a trav√©s de `argparse` con subparsers:

`sync` es el comando principal de operaci√≥n. Escanea recursivamente el directorio `documents/` buscando archivos con extensiones soportadas, compara hashes MD5 contra el √≠ndice existente, procesa archivos nuevos o modificados, elimina chunks de archivos borrados, y persiste el √≠ndice actualizado. La salida reporta conteos de archivos agregados, actualizados y eliminados.

`add <archivo>` permite ingestar un archivo espec√≠fico sin necesidad de ubicarlo en el directorio `documents/`. Valida la extensi√≥n, procesa el archivo, indexa los chunks resultantes y actualiza el √≠ndice.

`remove <fuente>` busca coincidencias parciales en los nombres de archivos indexados y elimina todos los chunks asociados a cada coincidencia.

`list` muestra todos los documentos indexados con su conteo de chunks y fecha de indexaci√≥n.

`search <consulta> [-n N]` ejecuta una b√∫squeda sem√°ntica y muestra los N resultados m√°s relevantes (por defecto 3) con su fuente, porcentaje de relevancia y un extracto de contenido.

`stats` presenta estad√≠sticas agregadas: documentos fuente, chunks totales, modelo de embeddings, fecha de √∫ltima sincronizaci√≥n, tama√±o en disco de la base de datos, y distribuci√≥n por tipo de archivo.

`rebuild [--force]` elimina completamente la base de datos vectorial (incluyendo el directorio `rag_data/`) y ejecuta una sincronizaci√≥n desde cero. Sin el flag `--force`, solicita confirmaci√≥n antes de proceder.

`check` verifica la disponibilidad de todas las dependencias del sistema: bibliotecas Python (chromadb, python-docx, pandas, openpyxl, pypdf, tabulate, httpx), disponibilidad del servidor Ollama, instalaci√≥n del modelo de embeddings, y existencia del directorio de documentos.

---

## 9. Gesti√≥n del Contexto Conversacional

### 9.1 Ventana de Contexto y Sumarizaci√≥n

El sistema implementa gesti√≥n inteligente del contexto conversacional para prevenir la degradaci√≥n de calidad que ocurre cuando el historial excede la capacidad del modelo. Se configura una ventana de contexto de 8192 tokens (`CONTEXT_WINDOW_SIZE`) y un l√≠mite de 20 mensajes (`MAX_HISTORY_MESSAGES`).

Cuando el historial supera el l√≠mite, la funci√≥n `manage_context` divide los mensajes en dos grupos: los 6 m√°s recientes (`KEEP_RECENT_MESSAGES`) se preservan intactos, mientras que los anteriores se condensan en un resumen generado por `qwen2.5:0.5b`. El resumen resultante se inyecta en el system prompt como "Resumen de la conversaci√≥n anterior", proporcionando contexto hist√≥rico sin consumir tokens de ventana de contexto.

### 9.2 Interacci√≥n entre Contexto Conversacional y RAG

La construcci√≥n del system prompt sigue un orden de prioridad deliberado. El contexto RAG se inyecta primero y se marca como "PRIORIDAD ALTA" porque la informaci√≥n documental verificada debe prevalecer sobre conversaciones previas y contexto general del usuario. El resumen conversacional se ubica despu√©s para proporcionar continuidad. Esta jerarqu√≠a asegura que, ante tokens limitados de contexto, los documentos RAG ocupen la posici√≥n m√°s favorable para influir en la generaci√≥n del modelo.

---

## 10. Ventajas de la Arquitectura Implementada

### 10.1 Privacidad y Soberan√≠a de Datos

Toda la infraestructura opera localmente. Los documentos corporativos nunca se transmiten a servicios externos. Los embeddings se generan en la m√°quina local mediante Ollama, la base vectorial reside en el sistema de archivos local, y la inferencia LLM ocurre en hardware propio. Esto cumple potencialmente con requerimientos de protecci√≥n de datos como GDPR o normativas sectoriales que proh√≠ben el procesamiento de informaci√≥n sensible en servidores de terceros.

### 10.2 Costo Operativo Nulo

Una vez descargados los modelos (una operaci√≥n que se realiza una sola vez), el sistema no genera costos recurrentes. No hay facturaci√≥n por token, por request, ni por almacenamiento en la nube. El costo se reduce al hardware local y la electricidad consumida, lo cual representa una ventaja significativa frente a servicios como la API de OpenAI donde cada consulta incrementa la factura.

### 10.3 Actualizaci√≥n Incremental de Conocimiento

Los documentos se pueden agregar, modificar o eliminar en cualquier momento. La sincronizaci√≥n incremental basada en hashes MD5 asegura que solo los archivos afectados se reprocesan, manteniendo tiempos de actualizaci√≥n proporcionales al cambio realizado y no al tama√±o total de la base documental.

### 10.4 Trazabilidad y Transparencia

Cada respuesta del modelo puede acompa√±arse de las fuentes documentales exactas que la sustentan, con indicaci√≥n del archivo, secci√≥n y porcentaje de relevancia. El usuario puede verificar la informaci√≥n expandiendo el panel de fuentes, lo que genera confianza y permite detectar posibles errores del modelo.

### 10.5 Modularidad y Desacoplamiento

Los procesadores de documentos, el motor de embeddings, la base vectorial, el servidor web y la interfaz de usuario operan como componentes independientes con interfaces bien definidas. El procesador de documentos puede extenderse a nuevos formatos agregando una funci√≥n al diccionario `PROCESSORS`. La funci√≥n de embeddings puede reemplazarse por otro modelo simplemente cambiando la variable `EMBEDDING_MODEL`. La interfaz de usuario puede consumir los endpoints RAG independientemente de c√≥mo se generen las respuestas.

---

## 11. Limitaciones Identificadas y Mejoras Propuestas

### 11.1 Limitaciones del MVP Actual

**Chunking est√°tico.** La estrategia de chunking actual utiliza un tama√±o fijo de 1000 caracteres con solapamiento de 100. Este enfoque no distingue entre tipos de contenido: un p√°rrafo narrativo y una tabla de ingredientes reciben el mismo tratamiento, aunque sus caracter√≠sticas sem√°nticas difieren significativamente. Un chunk que corta una tabla por la mitad pierde la coherencia de los datos que contiene.

**Ausencia de re-ranking.** Los resultados de la b√∫squeda vectorial se filtran √∫nicamente por un umbral de distancia coseno. No existe una etapa de re-ranking que utilice un modelo cross-encoder para refinar la relevancia de los candidatos recuperados. Los embeddings biencoder (como `nomic-embed-text`) son eficientes para la recuperaci√≥n inicial pero menos precisos que un cross-encoder para determinar la relevancia fina entre query y documento.

**Modelo de embeddings monoling√ºe.** Aunque `nomic-embed-text` funciona razonablemente bien con espa√±ol, est√° optimizado principalmente para ingl√©s. Textos con mezcla de idiomas, tecnicismos culinarios o jerga regional pueden no representarse con la misma fidelidad sem√°ntica.

**Sin OCR ni procesamiento de im√°genes en documentos.** Los archivos PDF que contienen texto como im√°genes escaneadas no se procesan correctamente porque `pypdf` solo extrae texto embebido, no realiza reconocimiento √≥ptico de caracteres.

**Ventana de contexto compartida.** El contexto RAG compite por tokens con el historial conversacional y el system prompt. En conversaciones largas con muchos documentos relevantes, los fragmentos RAG podr√≠an truncarse o no entrar completos.

**Single-threaded para embeddings.** La generaci√≥n de embeddings durante la ingesta se realiza secuencialmente, documento por documento. Para bases documentales grandes, esto puede resultar lento.

### 11.2 Mejoras Propuestas para Iteraciones Futuras

**Chunking sem√°ntico adaptativo.** Implementar un chunker que analice la estructura del contenido para decidir los puntos de corte. Las tablas deber√≠an mantenerse como unidades at√≥micas. Los procedimientos paso a paso deber√≠an conservarse completos. Los p√°rrafos narrativos podr√≠an dividirse en oraciones usando un tokenizador de lenguaje natural como `spaCy`.

**Pipeline de re-ranking con cross-encoder.** Agregar una etapa posterior a la b√∫squeda vectorial que utilice un modelo cross-encoder (como `cross-encoder/ms-marco-MiniLM-L-6-v2`) para revaluar y reordenar los candidatos. Esto mejorar√≠a significativamente la precisi√≥n de los resultados, especialmente para consultas ambiguas.

**Embeddings multiling√ºes.** Reemplazar `nomic-embed-text` por un modelo de embeddings optimizado para espa√±ol o multiling√ºe como `multilingual-e5-large` o `paraphrase-multilingual-MiniLM-L12-v2`, que ofrece representaciones sem√°nticas m√°s precisas para textos en espa√±ol.

**Integraci√≥n de OCR.** Incorporar `Tesseract` o `EasyOCR` para procesar PDFs escaneados, permitiendo que documentos hist√≥ricos digitalizados se incorporen a la base de conocimiento.

**Ingesta as√≠ncrona y en paralelo.** Refactorizar la ingesta para generar embeddings en batch asincr√≥nicamente, aprovechando la capacidad de Ollama de procesar m√∫ltiples textos en una sola llamada al endpoint `/api/embed` con el par√°metro `input` como lista.

**Panel de administraci√≥n web.** Desarrollar una interfaz web integrada en el frontend para gestionar la base de conocimiento: subir documentos, monitorear el estado de indexaci√≥n, ejecutar b√∫squedas de prueba y visualizar estad√≠sticas, eliminando la dependencia de la CLI.

**Soporte para metadatos enriquecidos.** Permitir que los usuarios asignen categor√≠as, etiquetas o niveles de confidencialidad a los documentos, habilitando filtros que restrinjan la b√∫squeda a subconjuntos espec√≠ficos de la base de conocimiento.

**Evaluaci√≥n cuantitativa del RAG.** Implementar un framework de evaluaci√≥n con datasets de test (pares pregunta-respuesta) que permita medir m√©tricas como precisi√≥n de recuperaci√≥n, recall, y calidad de la respuesta generada, para guiar las decisiones de optimizaci√≥n con datos objetivos.

---

## 12. Requerimientos M√≠nimos e Instrucciones de Despliegue

### 12.1 Requisitos de Hardware

El sistema requiere una computadora con al menos 8 GB de RAM para ejecutar los modelos m√°s ligeros (gemma3:4b, qwen2.5:0.5b y nomic-embed-text simult√°neamente). Para utilizar modelos m√°s grandes como phi4:latest (14B par√°metros), se recomiendan 16 GB de RAM o superior. El almacenamiento necesario es de aproximadamente 5 GB para los modelos base y 500 MB adicionales por cada 10,000 chunks indexados en ChromaDB. No se requiere GPU dedicada, aunque su presencia acelera significativamente la inferencia y la generaci√≥n de embeddings.

### 12.2 Requisitos de Software

El entorno requiere Python 3.10 o superior, Node.js 18 o superior para el frontend React, y Ollama instalado como servicio local. Las dependencias de Python se instalan mediante:

```bash
pip install fastapi uvicorn httpx numpy scipy
pip install openai-whisper
pip install chromadb python-docx pandas openpyxl pypdf tabulate
```

Las dependencias del frontend se instalan mediante:

```bash
cd chatbot-ui
npm install
```

### 12.3 Configuraci√≥n Inicial de Modelos

Antes de utilizar el sistema, los modelos de Ollama deben descargarse:

```bash
# Modelo de embeddings (obligatorio para RAG)
ollama pull nomic-embed-text

# Modelos de inferencia
ollama pull gemma3:4b        # Chat general
ollama pull phi4:latest      # Razonamiento matem√°tico
ollama pull qwen2.5:0.5b     # Router/clasificador (ultra ligero)

# Modelo de visi√≥n (opcional, para an√°lisis de im√°genes)
ollama pull llava:7b
```

### 12.4 Preparaci√≥n de la Base de Conocimiento

Los documentos fuente se colocan en el directorio `documents/`, que soporta subdirectorios organizados seg√∫n la taxonom√≠a que el usuario considere apropiada. El escaneo recursivo mediante `rglob("*")` explora toda la jerarqu√≠a de carpetas autom√°ticamente.

```bash
# Verificar dependencias
python rag_admin.py check

# Sincronizar documentos
python rag_admin.py sync

# Verificar indexaci√≥n
python rag_admin.py stats

# Probar b√∫squeda
python rag_admin.py search "ingredientes del pollo juanillo"
```

### 12.5 Inicio de Servicios

El sistema requiere tres procesos ejecut√°ndose simult√°neamente:

```bash
# Terminal 1: Servidor Ollama (si no est√° activo como servicio del sistema)
ollama serve

# Terminal 2: Servidor backend FastAPI
python -m uvicorn gemma_server:app --host 0.0.0.0 --port 8000

# Terminal 3: Frontend React
cd chatbot-ui
npm run dev
```

La interfaz estar√° disponible en `http://localhost:5173` y se comunicar√° con el backend en `http://localhost:8000`.

### 12.6 Verificaci√≥n de Funcionamiento

Para confirmar que el sistema RAG opera correctamente:

```bash
# Verificar el estado del RAG desde la API
curl http://localhost:8000/rag/status

# Probar una b√∫squeda RAG desde la API
curl -X POST "http://localhost:8000/rag/search?query=pollo+juanillo&n_results=3"

# Verificar la salud general del sistema
curl http://localhost:8000/health
```

Una respuesta correcta del endpoint `/rag/status` mostrar√° `"enabled": true`, `"initialized": true`, junto con los conteos de documentos y chunks indexados.

---

## 13. Estructura de Archivos del Proyecto

```
proyecto/
‚îú‚îÄ‚îÄ gemma_server.py              # Servidor FastAPI principal (orquestador)
‚îú‚îÄ‚îÄ rag_module.py                # N√∫cleo RAG: embeddings, ChromaDB, b√∫squeda
‚îú‚îÄ‚îÄ document_processors.py       # Extracci√≥n y chunking de documentos
‚îú‚îÄ‚îÄ rag_admin.py                 # CLI de administraci√≥n de la base de conocimiento
‚îú‚îÄ‚îÄ requirements.txt             # Dependencias Python
‚îú‚îÄ‚îÄ .gitignore                   # Exclusiones (rag_data/, documents/*, .venv/)
‚îÇ
‚îú‚îÄ‚îÄ documents/                   # Directorio de documentos fuente
‚îÇ   ‚îú‚îÄ‚îÄ README.md                # Instrucciones de uso
‚îÇ   ‚îî‚îÄ‚îÄ recetas/                 # Ejemplo: subdirectorio tem√°tico
‚îÇ       ‚îî‚îÄ‚îÄ pollo juanillo/      # Subdirectorio por receta
‚îÇ           ‚îú‚îÄ‚îÄ Receta.xlsx
‚îÇ           ‚îú‚îÄ‚îÄ Checklist.docx
‚îÇ           ‚îî‚îÄ‚îÄ Guion.docx
‚îÇ
‚îú‚îÄ‚îÄ rag_data/                    # Base de datos vectorial (persistente, no versionada)
‚îÇ   ‚îú‚îÄ‚îÄ chroma.sqlite3           # Almacenamiento ChromaDB
‚îÇ   ‚îî‚îÄ‚îÄ .index.json              # √çndice de archivos procesados con hashes
‚îÇ
‚îî‚îÄ‚îÄ chatbot-ui/                  # Frontend React
    ‚îú‚îÄ‚îÄ package.json
    ‚îú‚îÄ‚îÄ vite.config.js
    ‚îî‚îÄ‚îÄ src/
        ‚îú‚îÄ‚îÄ App.jsx              # Componente principal con integraci√≥n RAG
        ‚îú‚îÄ‚îÄ App.css
        ‚îú‚îÄ‚îÄ main.jsx
        ‚îî‚îÄ‚îÄ index.css
```

---

## 14. Consideraciones de Seguridad

### 14.1 Aislamiento de Datos

La base de datos vectorial y los documentos fuente residen exclusivamente en el sistema de archivos local. No se implementa autenticaci√≥n en los endpoints del servidor porque el despliegue previsto es estrictamente local (`localhost`). Si el sistema se expusiera en una red, ser√≠a imperativo agregar autenticaci√≥n JWT o API keys en los endpoints RAG y restringir los or√≠genes CORS a dominios espec√≠ficos.

### 14.2 Sanitizaci√≥n de Contenido

Los textos extra√≠dos de documentos se procesan tal cual, sin sanitizaci√≥n contra inyecci√≥n de prompts. Un documento maliciosamente construido podr√≠a contener instrucciones que el modelo interpretar√≠a como directivas, potencialmente alterando su comportamiento. En un entorno donde los documentos provienen exclusivamente de fuentes confiables internas, este riesgo es aceptable, pero deber√≠a abordarse si la plataforma se abre a documentos de terceros.

### 14.3 Control de Acceso a Documentos

Actualmente no existe control de acceso granular a los documentos indexados. Cualquier consulta puede acceder a cualquier chunk de la base de conocimiento. En entornos con documentos de diferentes niveles de confidencialidad, se deber√≠a implementar un sistema de roles y filtros de metadata que restrinja qu√© documentos son accesibles para cada usuario o grupo.

---

## 15. M√©tricas y Observabilidad

### 15.1 Logging Actual

El sistema implementa logging en consola utilizando `print()` con prefijos sem√°nticos: `üìö` para operaciones RAG, `üîÄ` para enrutamiento, `üìù` para sumarizaci√≥n, `‚úÇÔ∏è` para truncamiento de historial, y `‚ö†Ô∏è` para advertencias. Cada request registra la consulta del usuario, el modelo seleccionado, la categor√≠a de enrutamiento, el conteo de documentos RAG relevantes encontrados, y la respuesta generada.

### 15.2 M√©tricas Recomendadas para Producci√≥n

Para monitorear la efectividad del sistema RAG en producci√≥n, se deber√≠an implementar las siguientes m√©tricas: tiempo de generaci√≥n de embeddings para consultas (latencia P50 y P99), tiempo de b√∫squeda ChromaDB, distribuci√≥n de relevancia de los resultados recuperados, ratio de consultas sin resultados RAG versus total, tiempo total de respuesta end-to-end incluyendo transcripci√≥n Whisper cuando aplica, y uso de memoria por parte de ChromaDB y los modelos cargados.

---

## 16. Conclusiones

El sistema RAG implementado demuestra la viabilidad de construir un asistente conversacional con conocimiento especializado utilizando exclusivamente infraestructura local y modelos de c√≥digo abierto. El pipeline completo ‚Äî desde la ingesta de documentos Word y Excel hasta la presentaci√≥n de respuestas con fuentes citadas en una interfaz web moderna ‚Äî opera sin dependencias de servicios cloud, garantizando la privacidad de los datos y eliminando costos recurrentes.

La principal complejidad t√©cnica del proyecto no residi√≥ en la arquitectura conceptual del RAG, que sigue un patr√≥n bien establecido en la industria, sino en la integraci√≥n pr√°ctica entre las diferentes versiones de las herramientas involucradas. La adaptaci√≥n de `OllamaEmbeddingFunction` para compatibilizar ChromaDB 1.5 con la API de embeddings de Ollama ilustra un desaf√≠o recurrente en sistemas que combinan m√∫ltiples bibliotecas de c√≥digo abierto en r√°pida evoluci√≥n: las interfaces cambian entre versiones minor, y la documentaci√≥n no siempre refleja los cambios oportunamente.

El MVP cumple con los objetivos funcionales establecidos: los documentos se indexan correctamente, las b√∫squedas sem√°nticas retornan resultados relevantes, las respuestas del modelo priorizan el conocimiento documental sobre la generaci√≥n especulativa, y la interfaz de usuario presenta las fuentes de forma transparente. Las mejoras identificadas ‚Äî chunking sem√°ntico, re-ranking, embeddings multiling√ºes y panel de administraci√≥n web ‚Äî representan optimizaciones incrementales que pueden implementarse sin modificar la arquitectura fundamental.
